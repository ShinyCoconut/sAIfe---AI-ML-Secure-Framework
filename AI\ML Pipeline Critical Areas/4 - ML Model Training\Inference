## 4 - ML Model Training\Inference
#### Typically, the step whose entity is being/has been trained on a set of data and generates inferences when provided with inputs

- Possible threats and/or vulnerabilities:

	- **Security Misconfiguration** - Running outdated/vulnerable DBMS versions, Running in debug mode and/or revealing error handling information, Not changing default keys and passwords, Leaving the database server externally accessible through the Internet
		- Solution: Using up-to-date software versions, Following industry standards and recommendations for DBMS configuration, Restricting database access to the internal/private network

	-- Training-focused
	--- Poisoning: tries to manipulate the model by altering the training data being fed to it
	---- Label manipulation: switches or alters labels in order to degrade the model's future performance
	---- Input manipulation
	----- Direct poisoning of learning inputs: alters features of the data being input in order to change the model
	----- Indirect poisoning of the learning inputs: poisons the data before pre-processing to disturb model training

	-- Inference-focused
	--- Exploratory: tries to induce determined outputs by varying the input provided
	--- Oracle: tries to extract the model itself by providing inputs, analyzing and combining their results. It gets much harder when there is no access to the probabilities returned by the model.
	--- Direct manipulation of model inputs: alters the feature values processed by the model to get different predictions
	---- Source-target misclassification/targeted: does so with the intention of getting a specific classification
	---- Simple misclassification/untargeted: does so with the intention of getting any different than optimal classification
	--- Indirect manipulation of model inputs: alters the feature values processed by the model to get different predictions
	--- Membership inference: tries to figure out whether a determined set of inputs was part of the model's training data in order to clone the model
	--- Training data extraction/model inversion: with access to the model and its predictions, attempts to reverse engineer the results to recover training data

	- Possible solutions/preventions
	-- [STRONG] Customized inputs on client-side that should pass - in a determined way - a validation on the model's server-side. This method would work similarly to the known public/private-key pair authentication: only a legitimate actor would possess the information on how to modify the input data being sent in a way that it would succeed the validation tests.
	-- [WEAK] When dealing with classifiers, hide the class probabilities and only output the most likely class as the result.
	-- [AVERAGE] Gradient masking: change the model's function from smooth to abrupt, to avoid suffering significant output differences caused by slight input changes.

(\*) - Item may have direct impact on the AI/ML aspect itself

References:

- https://www.datasunrise.com/potential-db-threats/10-common-vulnerabilities/
- https://www.darkreading.com/vulnerabilities-threats/the-10-most-common-database-vulnerabilities
